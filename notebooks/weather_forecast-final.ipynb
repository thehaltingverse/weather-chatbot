{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Weather Forecast Synthesizer--Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook synthesizes weather forecasts from multiple APIs and uses an LLM to summarize and compare against historical data.\n",
    "- about the branching of capabilities and how it ties over to the other portions of the assignment.\n",
    "- talk about the goal of the workflow\n",
    "- additional details on artchitecture and decisions can ber found in report.\n",
    "\n",
    "## Important Notes\n",
    "- To support this application, data is pulled from various sources, usually leveraging an API key or token. These keys/tokens were obtained using my personal email and will be kept active through the evaluation period.\n",
    "- **Please note: that queries using the Open AI API key are a paid service and running that portion of the application will incur a small, nominal charge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "Set-up python environment and notebook with necessary packages and functions.\n",
    "\n",
    "### Instructions\n",
    "1. Ensure that environment has required libraries. If necessary, uncomment the cell with dependencies and install packages.\n",
    "2. If application folders structure is maintained upon transfer, then the file paths should not need updating. If the directories have changed, update file paths accordingly.\n",
    "3. API keys and tokens are included in the .env file. If errors appear when resolving, obtain separate keys or contact Ian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install dependencies\n",
    "# %pip install openmeteo-requests geopy transformers datasets torch python-dotenv openai protobuf accelerate sentencepiece huggingface_hub llama-cpp-python fastapi rich praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export environment packages and libraries\n",
    "# with open(\"requirements.txt\", \"w\") as f:\n",
    "#     subprocess.run([sys.executable, \"-m\", \"pip\", \"freeze\"], stdout=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load libraries \n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta, date, timezone\n",
    "import time\n",
    "import pandas as pd\n",
    "import openai\n",
    "import pprint\n",
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List, Dict\n",
    "import praw\n",
    "\n",
    "\n",
    "notebook_dir = Path().resolve()  # This is the notebook's directory\n",
    "env_path = notebook_dir.parent / \"env\" / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Load the API keys\n",
    "WEATHERBIT_API_KEY = os.getenv(\"WEATHERBIT_API_KEY\")\n",
    "NOAA_TOKEN = os.getenv(\"NOAA_TOKEN\")\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "NEWSAPI_API_KEY = os.getenv(\"NEWSAPI_API_KEY\")\n",
    "REDDIT_SECRET = os.getenv(\"REDDIT_SECRET\")\n",
    "REDDIT_ID = os.getenv(\"REDDIT_ID\")\n",
    "\n",
    "\n",
    "# Initialize OpenAI endpoint\n",
    "client = openai.OpenAI(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "These helper functions are defined explicitly within this notebook for reference and use. They have also been refactored and transferred to the corresponding module.py files for use by the MCP server, but should otherwise work exactly the same. These functions were created to perform three general tasks: \n",
    "1. Obtain the necessary data,\n",
    "2. Wrangle weather data into three tables, and\n",
    "3. Create the necessary artifacts and execute a query to the LLM.\n",
    "\n",
    "### Instructions\n",
    "1. Run cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to obtain weather data\n",
    "def get_coordinates(city_name):\n",
    "    geolocator = Nominatim(user_agent=\"weather_forecast\")\n",
    "    location = geolocator.geocode(city_name)\n",
    "    \n",
    "    if not location:\n",
    "        print(\"Location not found.\")\n",
    "        return None,None\n",
    "    \n",
    "    return location.latitude, location.longitude\n",
    "\n",
    "def get_weatherbit_forecast(lat, lon):\n",
    "\n",
    "    url = \"https://api.weatherbit.io/v2.0/forecast/daily\"\n",
    "    params = {\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "        \"key\": WEATHERBIT_API_KEY,\n",
    "        \"days\": 7\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Weatherbit API error:\", response.status_code)\n",
    "        return {}\n",
    "\n",
    "def get_open_meteo_forecast(lat, lon):\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"daily\": [\n",
    "            \"temperature_2m_max\",\n",
    "            \"temperature_2m_min\",\n",
    "            \"precipitation_sum\",\n",
    "            \"windspeed_10m_max\"\n",
    "        ],\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Open-Meteo API error:\", response.status_code)\n",
    "        return {}\n",
    "\n",
    "def safe_noaa_request(url, headers, params, max_retries=5, backoff=5):\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        elif response.status_code == 503:\n",
    "            print(f\"503 error, retrying in {backoff} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "        else:\n",
    "            print(f\"NOAA API error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def generate_past_10yr_ranges(days_back=7):\n",
    "    today = date.today()\n",
    "    ranges = []\n",
    "    for i in range(1, 11):\n",
    "        try:\n",
    "            start = today.replace(year=today.year - i)\n",
    "        except ValueError:\n",
    "            # Handle leap year case for Feb 29 by falling back to Feb 28\n",
    "            start = today.replace(year=today.year - i, day=28)\n",
    "        end = start + timedelta(days=days_back)\n",
    "        ranges.append((start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")))\n",
    "    return ranges\n",
    "\n",
    "def find_nearest_station(lat, lon, start_date, end_date):\n",
    "    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/stations\"\n",
    "    headers = {\"token\": NOAA_TOKEN}\n",
    "    params = {\n",
    "        \"datasetid\": \"GHCND\",\n",
    "        \"startdate\": start_date,\n",
    "        \"enddate\": end_date,\n",
    "        \"limit\": 1000,\n",
    "        # Slightly bigger bounding box to catch nearby stations\n",
    "        \"extent\": f\"{lat - 1},{lon - 1},{lat + 1},{lon + 1}\",\n",
    "        \"sortfield\": \"datacoverage\",\n",
    "        \"sortorder\": \"desc\"\n",
    "    }\n",
    "\n",
    "    response = safe_noaa_request(url, headers, params)\n",
    "    if response and response.status_code == 200:\n",
    "        stations = response.json().get(\"results\", [])\n",
    "        # Filter to only USW stations\n",
    "        usw_stations = [s for s in stations if s[\"id\"].startswith(\"GHCND:USW\")]\n",
    "\n",
    "        if not usw_stations:\n",
    "            print(\"No USW stations found in bounding box.\")\n",
    "            return None\n",
    "\n",
    "        # Debug print to see what was found\n",
    "        print(f\"Found {len(usw_stations)} USW stations, choosing closest...\")\n",
    "\n",
    "        # Find closest USW station by geodesic distance\n",
    "        closest_usw_station = min(\n",
    "            usw_stations,\n",
    "            key=lambda s: geodesic((lat, lon), (s[\"latitude\"], s[\"longitude\"])).km\n",
    "        )\n",
    "\n",
    "        print(f\"Closest USW station: {closest_usw_station['id']} - {closest_usw_station.get('name', '')}\")\n",
    "        return closest_usw_station[\"id\"]\n",
    "\n",
    "    else:\n",
    "        print(f\"NOAA API request failed with status: {response.status_code if response else 'No response'}\")\n",
    "    return None\n",
    "\n",
    "def get_noaa_data_for_range(station_id, start_date, end_date, datatypeids=None):\n",
    "    if datatypeids is None:\n",
    "        datatypeids = [\"TMIN\", \"TMAX\", \"PRCP\", \"AWND\"]\n",
    "\n",
    "    url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "    headers = {\"token\": NOAA_TOKEN}\n",
    "    all_results = []\n",
    "    limit = 1000\n",
    "    offset = 1\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"datasetid\": \"GHCND\",\n",
    "            \"datatypeid\": datatypeids,\n",
    "            \"stationid\": station_id,\n",
    "            \"startdate\": start_date,\n",
    "            \"enddate\": end_date,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "            \"units\": \"standard\",\n",
    "            \"sortfield\": \"date\",\n",
    "            \"sortorder\": \"asc\",\n",
    "            \"includemetadata\": \"false\"\n",
    "        }\n",
    "        response = safe_noaa_request(url, headers, params)\n",
    "        if not response:\n",
    "            break\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        all_results.extend(results)\n",
    "        metadata = data.get(\"metadata\", {}).get(\"resultset\", {})\n",
    "        count = metadata.get(\"count\", 0)\n",
    "        if offset + limit > count:\n",
    "            break\n",
    "        offset += limit\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def get_noaa_10yr_historical(lat, lon, days_back=7):\n",
    "    date_ranges = generate_past_10yr_ranges(days_back)\n",
    "    # Use earliest date range (10 years ago) to find station with coverage\n",
    "    earliest_start, earliest_end = date_ranges[-1]\n",
    "    station_id = find_nearest_station(lat, lon, earliest_start, earliest_end)\n",
    "    if not station_id:\n",
    "        raise ValueError(\"No NOAA station found with data coverage for the location and date range.\")\n",
    "\n",
    "    print(f\"Using station {station_id}\")\n",
    "\n",
    "    combined_results = []\n",
    "    for start_date, end_date in date_ranges:\n",
    "        print(f\"Fetching data for {start_date} to {end_date}...\")\n",
    "        data = get_noaa_data_for_range(station_id, start_date, end_date)\n",
    "        combined_results.extend(data)\n",
    "\n",
    "    return combined_results, station_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to obtain news data\n",
    "US_STATE_ABBR_TO_NAME = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee',\n",
    "    'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
    "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
    "    'DC': 'District of Columbia'\n",
    "}\n",
    "\n",
    "def extract_city_state(location: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts the city name and state name from a location string formatted as\n",
    "    'City', 'City, State', or 'City, State, Country'. Converts US state abbreviations\n",
    "    to full state names.\n",
    "\n",
    "    Args:\n",
    "        location (str): A location string, e.g., 'Phoenix, AZ' or 'Los Angeles, CA, USA'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: A tuple containing the extracted city name and full state name.\n",
    "                         If no state is found, returns empty string for state.\n",
    "                         Example: ('Phoenix', 'Arizona'), ('Los Angeles', 'California'), ('London', '')\n",
    "    \"\"\"\n",
    "    if not location or not isinstance(location, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    parts = [part.strip() for part in location.split(\",\") if part.strip()]\n",
    "    city = parts[0] if len(parts) >= 1 else \"\"\n",
    "    state = parts[1].upper() if len(parts) >= 2 else \"\"\n",
    "\n",
    "    # Convert state abbreviation to full name if found\n",
    "    full_state = US_STATE_ABBR_TO_NAME.get(state, parts[1] if len(parts) >= 2 else \"\")\n",
    "\n",
    "    return city, full_state\n",
    "\n",
    "def get_weather_news(city: str, api_key: str, days_back: int = 3, max_articles: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Queries the NewsAPI for weather-related news articles about a given city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The city to search news for. Accepts formats like 'City, State'.\n",
    "        api_key (str): Your NewsAPI.org API key.\n",
    "        days_back (int): How many days back to search for news.\n",
    "        max_articles (int): Maximum number of articles to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of formatted article summaries.\n",
    "    \"\"\"\n",
    "    # Extract the city name to improve search relevance\n",
    "    clean_city,clean_state = extract_city_state(city)\n",
    "    print(clean_city, clean_state)\n",
    "    \n",
    "    # Build query\n",
    "    # query = f\"{clean_city} weather OR storm OR rainfall OR heat OR climate OR flood\"\n",
    "    \n",
    "    query = (\n",
    "        f\"({clean_city} OR {clean_state})\"\n",
    "        \"AND (weather OR storm OR forecast OR temperature OR rainfall OR snow OR flooding OR humidity) \"\n",
    "        \"-sports -baseball -football -NBA -concert -game -soccer -crime\"\n",
    "    )\n",
    "\n",
    "    # Dates\n",
    "    from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "    to_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"from\": from_date,\n",
    "        \"to\": to_date,\n",
    "        \"language\": \"en\",\n",
    "        \"sortBy\": \"relevancy\",\n",
    "        \"pageSize\": max_articles,\n",
    "        \"apiKey\": api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"title\": article.get(\"title\"),\n",
    "                \"source\": article.get(\"source\", {}).get(\"name\"),\n",
    "                \"datePublished\": article.get(\"publishedAt\"),\n",
    "                \"snippet\": article.get(\"description\"),\n",
    "                \"url\": article.get(\"url\")\n",
    "            }\n",
    "            for article in articles if article.get(\"title\") and article.get(\"description\")\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[NewsAPI Error] {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Functions to obtain social media data\n",
    "def fetch_reddit_weather_posts(\n",
    "    reddit_client_id: str,\n",
    "    reddit_client_secret: str,\n",
    "    reddit_user_agent: str,\n",
    "    location: str,\n",
    "    max_posts: int = 20,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch recent weather-related Reddit posts from select subreddits and subreddits matching the location name.\n",
    "\n",
    "    Args:\n",
    "        reddit_client_id (str): Reddit API client ID.\n",
    "        reddit_client_secret (str): Reddit API client secret.\n",
    "        reddit_user_agent (str): User agent string.\n",
    "        location (str): City or city+state string, e.g. \"Seattle WA\"\n",
    "        max_posts (int): Maximum number of posts to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of posts dicts with keys: title, subreddit, created_utc, url, selftext\n",
    "    \"\"\"\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_client_id,\n",
    "        client_secret=reddit_client_secret,\n",
    "        user_agent=reddit_user_agent,\n",
    "    )\n",
    "\n",
    "    weather_subreddits = [\"weather\", \"climate\", \"StormComing\"]\n",
    "\n",
    "    location_parts = location.lower().replace(\",\", \"\").split()\n",
    "    location_subreddits = []\n",
    "\n",
    "    for part in location_parts:\n",
    "        # Search subreddits with location part in the name, limit 5 per part\n",
    "        for sub in reddit.subreddits.search_by_name(part, exact=False)[:5]:\n",
    "            sub_name = sub.display_name.lower()\n",
    "            if any(loc_part in sub_name for loc_part in location_parts):\n",
    "                location_subreddits.append(sub.display_name)\n",
    "\n",
    "    # Unique combined list\n",
    "    all_subreddits = list(set(weather_subreddits + location_subreddits))\n",
    "\n",
    "    weather_keywords = [\n",
    "        \"weather\", \"storm\", \"flood\", \"rain\", \"snow\", \"heatwave\",\n",
    "        \"tornado\", \"hurricane\", \"drought\", \"lightning\", \"climate\",\n",
    "        \"hail\", \"wind\"\n",
    "    ]\n",
    "\n",
    "    posts = []\n",
    "\n",
    "    for subreddit_name in all_subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        query = \" OR \".join(weather_keywords)\n",
    "        # Search top posts in past week, sorted by relevance\n",
    "        for submission in subreddit.search(query, time_filter=\"week\", sort=\"relevance\", limit=max_posts):\n",
    "            posts.append({\n",
    "                \"title\": submission.title,\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"url\": submission.url,\n",
    "                \"selftext\": submission.selftext,\n",
    "            })\n",
    "            if len(posts) >= max_posts:\n",
    "                break\n",
    "        if len(posts) >= max_posts:\n",
    "            break\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to manipulate data\n",
    "def normalize_forecast(data,source_name):\n",
    "    normalized = []\n",
    "\n",
    "    if source_name == \"open_meteo\":\n",
    "        daily = data.get(\"daily\", {})\n",
    "        dates = daily.get(\"time\", [])\n",
    "        for i, date in enumerate(dates):\n",
    "            normalized.append({\n",
    "                \"date\": date,\n",
    "                \"temp_max-degC-open_meteo\": daily.get(\"temperature_2m_max\", [None]*len(dates))[i],\n",
    "                \"temp_min-degC-open_meteo\": daily.get(\"temperature_2m_min\", [None]*len(dates))[i],\n",
    "                \"precip-mm-open_meteo\": daily.get(\"precipitation_sum\", [None]*len(dates))[i],\n",
    "                \"wind_max-mpersec-open_meteo\": daily.get(\"windspeed_10m_max\", [None]*len(dates))[i]\n",
    "            })\n",
    "\n",
    "    elif source_name == \"weatherbit\":\n",
    "        for day in data.get(\"data\", []):\n",
    "            normalized.append({\n",
    "                \"date\": day.get(\"datetime\"),\n",
    "                \"temp_max-degC-weatherbit\": day.get(\"max_temp\"),\n",
    "                \"temp_min-degC-weatherbit\": day.get(\"min_temp\"),\n",
    "                \"precip-mm-weatherbit\": day.get(\"precip\"),\n",
    "                \"wind_max-mpersec-weatherbit\": day.get(\"wind_spd\")\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(normalized)\n",
    "\n",
    "def merge_forecasts(open_meteo_data, weatherbit_data, normalize_fn):\n",
    "    # Normalize each source\n",
    "    df_openmeteo = normalize_fn(open_meteo_data, source_name=\"open_meteo\")\n",
    "    df_weatherbit = normalize_fn(weatherbit_data, source_name=\"weatherbit\")\n",
    "\n",
    "    # Merge on date using outer join to retain all data points\n",
    "    merged_df = pd.merge(df_openmeteo, df_weatherbit, on=\"date\", how=\"outer\")\n",
    "\n",
    "    # Sort by date\n",
    "    merged_df = merged_df.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def normalize_noaa_data(noaa_raw_data):\n",
    "    \"\"\"\n",
    "    Normalize NOAA raw data into a pandas DataFrame matching your forecast column naming.\n",
    "    \n",
    "    Args:\n",
    "        noaa_raw_data (list of dict): raw NOAA data list\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: normalized DataFrame with columns:\n",
    "                      ['date', 'temp_max-degC-noaa', 'temp_min-degC-noaa', 'precip-mm-noaa', 'wind_max-mpersec-noaa']\n",
    "    \"\"\"\n",
    "    if not noaa_raw_data:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"date\",\n",
    "            \"temp_max-degC-noaa\",\n",
    "            \"temp_min-degC-noaa\",\n",
    "            \"precip-mm-noaa\",\n",
    "            \"wind_max-mpersec-noaa\"\n",
    "        ])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(noaa_raw_data)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Pivot so each datatype is a column\n",
    "    df_pivot = df.pivot_table(index='date', \n",
    "                              columns='datatype', \n",
    "                              values='value', \n",
    "                              aggfunc='first').reset_index()\n",
    "\n",
    "    # Rename columns with your naming convention\n",
    "    df_pivot = df_pivot.rename(columns={\n",
    "        \"TMAX\": \"temp_max-degC-noaa\",\n",
    "        \"TMIN\": \"temp_min-degC-noaa\",\n",
    "        \"PRCP\": \"precip-mm-noaa\",\n",
    "        \"AWND\": \"wind_max-mpersec-noaa\"\n",
    "    })\n",
    "\n",
    "    # Ensure all columns exist\n",
    "    expected_cols = [\n",
    "        \"date\",\n",
    "        \"temp_max-degC-noaa\",\n",
    "        \"temp_min-degC-noaa\",\n",
    "        \"precip-mm-noaa\",\n",
    "        \"wind_max-mpersec-noaa\"\n",
    "    ]\n",
    "    for col in expected_cols:\n",
    "        if col not in df_pivot.columns:\n",
    "            df_pivot[col] = pd.NA\n",
    "\n",
    "    # Reorder columns\n",
    "    df_pivot = df_pivot[expected_cols]\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "def summarize_noaa_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize NOAA historical weather data by computing mean, std, and count \n",
    "    for each weather variable, handling missing data appropriately.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): NOAA normalized DataFrame with expected columns:\n",
    "            'temp_max-degC-noaa', 'temp_min-degC-noaa', \n",
    "            'precip-mm-noaa', 'wind_max-mpersec-noaa'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary with average, standard deviation, and count\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure date is datetime\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Define weather columns to summarize\n",
    "    weather_cols = [\n",
    "        \"temp_max-degC-noaa\",\n",
    "        \"temp_min-degC-noaa\",\n",
    "        \"precip-mm-noaa\",\n",
    "        \"wind_max-mpersec-noaa\"\n",
    "    ]\n",
    "\n",
    "    # Drop rows where all weather columns are missing\n",
    "    df_clean = df.dropna(subset=weather_cols, how=\"all\")\n",
    "\n",
    "    # Ensure all columns are numeric (e.g., convert <NA> to np.nan)\n",
    "    for col in weather_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "    # Create summary stats\n",
    "    summary = pd.DataFrame({\n",
    "        \"mean\": df_clean[weather_cols].mean(),\n",
    "        \"std\": df_clean[weather_cols].std(),\n",
    "        \"count\": df_clean[weather_cols].count()\n",
    "    })\n",
    "\n",
    "    # Round results for clarity\n",
    "    summary = summary.round(2)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_noaa_daily_climatology(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute average, standard deviation, and count for each day-of-year across 10 years\n",
    "    of NOAA weather data.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): NOAA normalized DataFrame with expected columns:\n",
    "            'date', 'temp_max-degC-noaa', 'temp_min-degC-noaa', \n",
    "            'precip-mm-noaa', 'wind_max-mpersec-noaa'\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary DataFrame with per-day statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure datetime format\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    \n",
    "    # Extract month and day to group by day-of-year\n",
    "    df[\"month_day\"] = df[\"date\"].dt.strftime(\"%m-%d\")\n",
    "\n",
    "    # Define weather columns\n",
    "    weather_cols = [\n",
    "        \"temp_max-degC-noaa\",\n",
    "        \"temp_min-degC-noaa\",\n",
    "        \"precip-mm-noaa\",\n",
    "        \"wind_max-mpersec-noaa\"\n",
    "    ]\n",
    "\n",
    "    # Ensure numeric and drop rows missing all variables\n",
    "    df_clean = df.dropna(subset=weather_cols, how=\"all\")\n",
    "    for col in weather_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "    # Group by month_day and calculate summary stats\n",
    "    summary = df_clean.groupby(\"month_day\")[weather_cols].agg(['mean', 'std', 'count'])\n",
    "\n",
    "    # Flatten multi-index columns\n",
    "    summary.columns = ['-'.join(col).strip() for col in summary.columns.values]\n",
    "    summary = summary.reset_index()\n",
    "\n",
    "    return summary\n",
    "\n",
    "def format_news_data(articles: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Formats a list of weather-related news articles into a readable summary\n",
    "    suitable for LLM prompting or report inclusion.\n",
    "\n",
    "    Args:\n",
    "        articles (List[Dict]): A list of dictionaries, each containing metadata\n",
    "                               for a news article with keys like 'title',\n",
    "                               'source', 'datePublished', 'snippet', and 'url'.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string summarizing article details. If no articles are\n",
    "             provided, returns a fallback message.\n",
    "    \"\"\"\n",
    "    if not articles:\n",
    "        return \"No weather-related news was found for this city in the past few days.\"\n",
    "\n",
    "    lines = [\"Recent Weather News Articles:\"]\n",
    "    for i, a in enumerate(articles, 1):\n",
    "        title = a.get(\"title\", \"No Title\")\n",
    "        source = a.get(\"source\", \"Unknown Source\")\n",
    "        date = a.get(\"datePublished\", \"Unknown Date\")\n",
    "        snippet = a.get(\"snippet\", \"No snippet available.\")\n",
    "        url = a.get(\"url\", \"No URL\")\n",
    "\n",
    "        lines.append(\n",
    "            f\"\\n[{i}] {title}\\n\"\n",
    "            f\"Source: {source} | Published: {date}\\n\"\n",
    "            f\"Snippet: {snippet}\\n\"\n",
    "            f\"Link: {url}\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def format_reddit_posts_for_llm(posts: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format Reddit posts for input to an LLM.\n",
    "\n",
    "    Args:\n",
    "        posts (List[Dict]): List of Reddit post dicts with keys 'title', 'subreddit', 'created_utc', 'url', 'selftext'.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted multi-line string summarizing posts.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    formatted_posts = []\n",
    "    for post in posts:\n",
    "        # Convert epoch UTC to readable datetime string\n",
    "        dt = datetime.utcfromtimestamp(post['created_utc']).strftime('%Y-%m-%d %H:%M UTC')\n",
    "        # Snippet from selftext, limit to 150 chars (or empty if no selftext)\n",
    "        snippet = (post['selftext'][:150] + '...') if post['selftext'] else ''\n",
    "        formatted = (\n",
    "            f\"[{dt}] r/{post['subreddit']}: {post['title']}\\n\"\n",
    "            f\"Snippet: {snippet}\\n\"\n",
    "            f\"URL: {post['url']}\\n\"\n",
    "            \"----\"\n",
    "        )\n",
    "        formatted_posts.append(formatted)\n",
    "\n",
    "    return \"\\n\".join(formatted_posts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create prompt and query LLM\n",
    "def create_chatgpt_prompt(persona: str, instructions: str, output_format: str, city: str, lat: str, lon: str, station_id: str, news: str, social_media: str,\n",
    "                         df1_str: str, df2_str: str, df3_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a prompt for ChatGPT with persona, instructions, output format,\n",
    "    and embedded dataframes as context.\n",
    "\n",
    "    Args:\n",
    "        persona (str): Description of assistant’s role and tone.\n",
    "        instructions (str): Specific tasks or questions for the model.\n",
    "        output_format (str): Desired format for the response.\n",
    "        df1_str (str): String representation (e.g., JSON or CSV) of first dataframe.\n",
    "        df2_str (str): String representation of second dataframe.\n",
    "        df3_str (str): String representation of third dataframe.\n",
    "\n",
    "    Returns:\n",
    "        str: The full prompt string to send to the ChatGPT API.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are {persona}.\n",
    "\n",
    "Your task is to:\n",
    "{instructions}\n",
    "\n",
    "Here are the datasets to assist your analysis:\n",
    "\n",
    "Dataset 1:\n",
    "{df1_str}\n",
    "\n",
    "Dataset 2:\n",
    "{df2_str}\n",
    "\n",
    "Dataset 3:\n",
    "{df3_str}\n",
    "\n",
    "News data:\n",
    "{news}\n",
    "\n",
    "Social media data:\n",
    "{social_media}\n",
    "\n",
    "Please provide your response strictly following this format:\n",
    "\n",
    "City: {city}\n",
    "Latitude: {lat}\n",
    "Longitude: {lon}\n",
    "NOAA Station ID: {station_id}\n",
    "\n",
    "{output_format}\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def query_openai(prompt, openai_key, model=\"gpt-4o\", temperature=0.7, max_tokens=1000):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\"\n",
    "                 ,\"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def query_llm_with_fallback(\n",
    "    prompt: str,\n",
    "    openai_key: str = None,\n",
    "    notebook_path: Path = None,\n",
    "    repo_id: str = \"TheBloke/OpenChat-3.5-1210-GGUF\",\n",
    "    model_filename: str = \"openchat-3.5-1210.Q4_K_M.gguf\",\n",
    "    max_tokens: int = 1500,\n",
    "    n_ctx: int = 8192,\n",
    "    n_threads: int = 8,\n",
    "    verbose: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Query OpenAI if API key is present. Otherwise, use a local GGUF model downloaded from Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to query.\n",
    "        openai_key (str, optional): OpenAI API key. If None, use local model.\n",
    "        notebook_path (Path, optional): Path to the notebook (use __file__ or Path.cwd() in scripts).\n",
    "        repo_id (str): Hugging Face repo ID for the GGUF model.\n",
    "        model_filename (str): GGUF model file name.\n",
    "        max_tokens (int): Max tokens for LLM response.\n",
    "        n_ctx (int): Context length for LLM.\n",
    "        n_threads (int): Threads used by Llama model.\n",
    "        verbose (bool): Verbosity for Llama.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's text output.\n",
    "    \"\"\"\n",
    "    if openai_key:\n",
    "        print(\"Querying OpenAI endpoint...\")\n",
    "        response = query_openai(prompt, openai_key=openai_key)  # Assumes this is defined elsewhere\n",
    "        return response\n",
    "    \n",
    "    print(\"No OpenAI key detected. Using local GGUF model.\")\n",
    "\n",
    "    # Determine model directory relative to the notebook\n",
    "    notebook_dir = notebook_path.parent if notebook_path else Path.cwd()\n",
    "    local_model_dir = notebook_dir / \"models\"\n",
    "    model_path = local_model_dir / model_filename\n",
    "\n",
    "    # Download model if not present\n",
    "    if not model_path.exists():\n",
    "        print(f\"Model not found locally. Downloading from Hugging Face...\")\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=model_filename,\n",
    "            local_dir=local_model_dir\n",
    "        )\n",
    "        print(f\"Download complete. Model saved at: {model_path}\")\n",
    "    else:\n",
    "        print(f\"Using cached model at: {model_path}\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Querying model...\")\n",
    "    llm = Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=n_ctx,\n",
    "        n_threads=n_threads,\n",
    "        use_mlock=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Query model\n",
    "    output = llm(prompt, max_tokens=max_tokens, echo=False)\n",
    "    return output[\"choices\"][0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "Given an input city, weather data is obtained from the following sources: Weatherbit, Open-Meteo, and NOAA. Their respective databases are queried using their defined API logic and the lat/lon coordinates of the input city. These raw data are then stored as pandas dataframes for wrangling.\n",
    "\n",
    "### Important Notes\n",
    "For this Proof-of-Concept application, a few constraints on the functionality were placed to balance development time and demonstration of capability:\n",
    "- Finding high-quality historical datasets usually required querying NOAA USW weather stations as other stations often had incomplete data. As such, the NOAA query was written to select the USW weather station with coordinates within a bounding box of the latitude/longitude coordinates of the input city and the highest proportion of complete data for the historical date ranges. If there is no station within the bounding box, the workflow will fail with an error message, and no summary forecast report will be produced. In a production application, more graceful failover logic can be implemented.\n",
    "- There has been limited testing of obtaining the appropriate data for cities outside of the United States.\n",
    "- Occasionally, the underlying databases will be unavailable. While some failover logic has been implemented for temporary outages, extended outages may require running at a later time when the databases are available.\n",
    "\n",
    "### Instructions\n",
    "1. Update with desired city located with the United States. Key data will be printed as the workflow progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for Seattle, WA: 47.6038321, -122.330062\n",
      "Found 12 USW stations, choosing closest...\n",
      "Closest USW station: GHCND:USW00024234 - SEATTLE BOEING FIELD, WA US\n",
      "Using station GHCND:USW00024234\n",
      "Fetching data for 2024-06-02 to 2024-06-09...\n",
      "Fetching data for 2023-06-02 to 2023-06-09...\n",
      "Fetching data for 2022-06-02 to 2022-06-09...\n",
      "Fetching data for 2021-06-02 to 2021-06-09...\n",
      "Fetching data for 2020-06-02 to 2020-06-09...\n",
      "Fetching data for 2019-06-02 to 2019-06-09...\n",
      "Fetching data for 2018-06-02 to 2018-06-09...\n",
      "Fetching data for 2017-06-02 to 2017-06-09...\n",
      "Fetching data for 2016-06-02 to 2016-06-09...\n",
      "Fetching data for 2015-06-02 to 2015-06-09...\n",
      "Seattle Washington\n"
     ]
    }
   ],
   "source": [
    "# Example inputs, with corresponding USW station for reference. Actual station used may differ\n",
    "# city = \"New York, NY\" #GHCND:USW00094728\n",
    "# city = \"Chicago, IL\" #GHCND:USW00094846\n",
    "# city = \"San Francisco, CA\" #GHCND:USW00023234\n",
    "city = \"Seattle, WA\" #GHCND:USW00024233\n",
    "# city = \"Atlanta, GA\" #GHCND:USW00013874\n",
    "# city = \"Minneapolis, MN\" #GHCND:USW00014922\n",
    "# city = \"Denver, CO\" #GHCND:USW00023062\n",
    "# city = \"Boston, MA\" #GHCND:USW00014739\n",
    "# city = \"Miami, FL\" #GHCND:USW00012839\n",
    "\n",
    "# Input city name and state, with following format: \"City, State Abbreviation\"\n",
    "# city = \"Phoenix, AZ\" #GHCND:USW00023183\n",
    "\n",
    "# Obtain and print lat/lon coordinates for selected city\n",
    "lat, lon = get_coordinates(city)\n",
    "print(f\"Coordinates for {city}: {lat}, {lon}\")\n",
    "\n",
    "# Fetch Weatherbit forecast data\n",
    "weatherbit_data = get_weatherbit_forecast(lat, lon)\n",
    "\n",
    "# Fetch Open-Meteo forecast data\n",
    "open_meteo_data = get_open_meteo_forecast(lat, lon)\n",
    "\n",
    "# Fetch NOAA historical data\n",
    "noaa_data, station_id = get_noaa_10yr_historical(lat, lon)\n",
    "\n",
    "# Fetch news data\n",
    "news_data = get_weather_news(city,api_key=NEWSAPI_API_KEY, max_articles=3)\n",
    "\n",
    "# Fetch Reddit posts\n",
    "reddit_data = fetch_reddit_weather_posts(\n",
    "    reddit_client_id=REDDIT_ID,\n",
    "    reddit_client_secret=REDDIT_SECRET,\n",
    "    reddit_user_agent=\"WeatherAnalysisBot/0.1 by OkHold2363\",\n",
    "    location=city,\n",
    "    max_posts=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "This section will take the raw dataframes produced in the previous step and normalize and combine them to produce three tables to be used as input for the final LLM prompt, namely:\n",
    "\n",
    "- forecast_df_merged: the forecast data from Open-Meteo and Weatherbit,\n",
    "- daily_historical_df: the daily 10-year historical data from the nearest NOAA station, summarized by day, and\n",
    "- summary_historical_df: the 10-year historical data from the nearest NOAA station, summarized by weather variable.\n",
    "\n",
    "While the raw datasets could have been used and analysis handled in the prompt, due to the constraints of the assignment, untested abilities of various LLMs for this task, and relative ease to wrangle, the data were manually transformed and provided to the LLM in a standard form that makes the definitions and assumptions more clear. \n",
    "\n",
    "### Instructions\n",
    "1. Run cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and merge forecast data\n",
    "forecast_df_merged = merge_forecasts(open_meteo_data, weatherbit_data, normalize_forecast)\n",
    "\n",
    "# Normalize historical data\n",
    "historical_df = normalize_noaa_data(noaa_data)\n",
    "\n",
    "# Summarize historical data\n",
    "summary_historical_df = summarize_noaa_data(historical_df)\n",
    "\n",
    "# Summarize daily historical data\n",
    "daily_historical_df = summarize_noaa_daily_climatology(historical_df)\n",
    "\n",
    "# Format news articles for LLM\n",
    "news_formatted = format_news_data(news_data)\n",
    "\n",
    "# Format social media posts for LLM\n",
    "reddit_post_formatted = format_reddit_posts_for_llm(reddit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "These steps create the necessary artifacts to be included in the final LLM prompt. The are created to be modular and allow for rapid iteration and testing, should the need occur. Note that this prompt is relatively long. Its content and the number of tokens required can be optimized within the context of the selected LLM model, its capabilities, and other considerations. For the purposes of this application POC, the decision was made to provide more explicit context to achieve a minimum quality of response.\n",
    "\n",
    "### Instructions\n",
    "1. Run cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifacts for prompt\n",
    "persona = \"You are a professional, friendly meteorologist, communicating with an audience about their local weather.\"\n",
    "instructions = \"\"\"\n",
    "        First, analyze the provided table of weather forecast data for the city in question. It has the data from multiple sources. Use your expertise and knowledge about the weather for that location to provide a single, 7-day forecast of the weather in a table format along with any helpful commentary. For example, in cases where there is a large discrepancy between the two provided forecasts for a particular variable, consider providing commentary on its presence, potential root cause, and how you resolved it for the final forecast.\n",
    "        Second, analyze and compare the summary historical data for the particular variable with the forecast data. \n",
    "        Third, analyze and compare the daily historical data for the particular variable on that date with the forecast data.\n",
    "        For the second and third tasks, Consider including anything noteworthy in the \"Historical comparison\" section, for example calling out large deviations for historical data. Use your well-informed, and expert opinion to decide when and how to highlight discrepancies. Is the forecast typical compared to history? Anything unusual? Look at different metrics like temperature, humidity, wind, wind chill, cloud cover, etc.\n",
    "        Fourth, for all of the weather variables, highlight important considerations that residents should take with regard to that variable including potential severe weather, unusual conditions, or impacts on daily life in the \"Important considerations\" section. For example, if there is extreme or unsafe temperatures, include a note to not leave children in cars, think about pets, and hydrate often.\n",
    "        Fifth, in the \"About the data\" section, list any data anomalies, limitations, or special considerations that had to be taken into account in your analysis (for example, averaging two different values for temperature). Also attribute the sources of your data here.\n",
    "        Sixth, the \"Weather-related news\" section: summarize the sentiment of the news articles that were found. Filter for only the most relevant to the weather and input location. Provide up to three high-quality news articles for reference.\n",
    "        Seventh, the \"Social media posts\" section: summarize the sentiment of the social media posts that were found. Filter for only the most relevant to the weather and input location. Provide up to three high-relevancy posts for reference.\n",
    "        If any data are conflicting or missing, please highlight and explain. If you are unsure of a conclusion, feel free to make it if you provide acknowledgement of limitations. If you don't know the answer, do not make one up or hallucinate a response; instead, acknowledge limitation(s) and recommend other actions to resolve. End your response politely and professionally.\n",
    "        \"\"\"\n",
    "output_format = f\"\"\"The final 7-day forecast should be formatted as a table. The table should have the forecast the date along the top of the table, and the rows should contain the predicted value for the particular variable.\n",
    "\n",
    "Example table:\n",
    "| Date                | 2025-05-28 | 2025-05-29 | 2025-05-30 | 2025-05-31 | 2025-06-01 | 2025-06-02 | 2025-06-03 |\n",
    "|---------------------|------------|------------|------------|------------|------------|------------|------------|\n",
    "| Max Temp (°C)       | 39.0       | 37.3       | 38.9       | 39.1       | 33.2       | 35.0       | 33.2       |\n",
    "| Min Temp (°C)       | 22.7       | 22.3       | 22.9       | 27.4       | 24.9       | 22.4       | 24.0       |\n",
    "| Precipitation (mm)  | 0.0        | 0.0        | 0.0        | 0.25       | 2.5        | 2.5        | 0.0        |\n",
    "| Max Wind Speed (m/s)| 8.9        | 7.3        | 10.0       | 17.4       | 18.6       | 18.5       | 15.6       |\n",
    "\n",
    "Below the table, include a commentary section, formatted as below with the following information:\n",
    "\n",
    "Commentary:\n",
    "**Historical comparison:**\n",
    "- individual points in bulleted list.\n",
    "\n",
    "**Important considerations:**\n",
    "- individual points in bulleted list.\n",
    "\n",
    "**About the data:**\n",
    "- individual points in bulleted list.\n",
    "\n",
    "**Weather-related news:**\n",
    "- Short summary of sentiment of recent weather-related news.\n",
    "- Three relevant news articles, if present.\n",
    "\n",
    "**Social media posts:**\n",
    "- Short summary of sentiment of recent social media posts.\n",
    "- Three relevant social media posts, including title and URL.\n",
    "\"\"\"\n",
    "df1_str = forecast_df_merged.to_json(orient='records') \n",
    "df2_str = summary_historical_df.to_json(orient='records')\n",
    "df3_str = daily_historical_df.to_json(orient='records')\n",
    "\n",
    "# Create prompt\n",
    "prompt = create_chatgpt_prompt(persona, instructions, output_format, city, lat, lon, station_id, news_formatted, reddit_post_formatted, df1_str, df2_str, df3_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLM directly from Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "This section tests the final prompt against an LLM. There are two options for LLM to query:\n",
    "1. an OpenAI endpoint, or\n",
    "2. a local GGUF model.\n",
    "\n",
    "An OpenAI key has been provided in the initial transfer in the .env file and instantiated in the Setup section above. Running the cell below as-is will result in a call to the OpenAI endpoint at a small, nominal charge.\n",
    "\n",
    "If you would like to test a local model, you will need to uncomment the line in the cell below that sets OPENAI_KEY = None. This will trigger a workflow that will look in the application file directory for a specific GGUF model. If the model is present, then the query will proceed with the generated prompt. Otherwise, the model will be downloaded from huggingface in a process that will require some time (depending on connection speed), prior to the query.\n",
    "\n",
    "The default model to download is the \"openchat-3.5-1210.Q4_K_M.gguf\". It was selected to demonstrate the capabilities of a local model that can be stored and run on a modestly powered CPU, free of charge, to produce serviceable results. \n",
    "\n",
    "### Instructions\n",
    "1. Adjust whether or not to use the OPENAI_KEY.\n",
    "2. Run cell.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query LLM\n",
    "\n",
    "# OPENAI_KEY = None # Uncomment to leverage a locally downloaded model. Download should initiate itself.\n",
    "# response = query_llm_with_fallback(openai_key=OPENAI_KEY, prompt=prompt, notebook_path=notebook_dir)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Weather Forecast Synthesizer--MCP Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The following cells are meant to demonstrate a few different methods to interact with the same workflow logic as above to provide a friendly weather forecast and analysis, but through an MCP server. The server will then use the logic to extract the data, find the appropriate model, and query with the created prompt. Updates to the logic will need to occur in the reference files/modules that are leverage by the MCP server in order to take effect.\n",
    "\n",
    "## Important Notes\n",
    "- If you want to run inference against a local model using the MCP server, you will need to update the .env file to remove the OPENAI_KEY variable.\n",
    "- To support this application, data is pulled from various sources, usually leveraging an API key or token. These keys/tokens were obtained using my personal email and will be kept active through the evaluation period.\n",
    "- **Please note: that queries using the Open AI API key are a paid service and running that portion of the application will incur a small, nominal charge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin-up MCP server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "Two methods are provided to spin up the associated MCP server.\n",
    "\n",
    "### Option 1 Instructions (spin up the MCP server from this notebook): \n",
    "1. Run the below cell.\n",
    "\n",
    "### Options 2 Instructions (spin up the MCP server from bash):\n",
    "1. Open a bash terminal. \n",
    "2. Navigate to the application root directory, /weatherChatbot.\n",
    "3. Activate virtual environment: source .venv/Scripts/activate\n",
    "4. Install any necessary packages (see pip install line at top or env/requirements.txt).\n",
    "5. Start FastAPI server: uvicorn mcp_server.main:app --reload\n",
    "6. Confirm that server is running by following the URL provided in the resulting response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCP Server started on http://127.0.0.1:8000\n",
      "✅ Response: 200\n",
      "{'message': 'MCP Weather Server is running!'}\n"
     ]
    }
   ],
   "source": [
    "# Step into project root (one level up from 'notebooks/')\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Start FastAPI server via uvicorn\n",
    "server = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"uvicorn\", \"mcp_server.main:app\", \"--port\", \"8000\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    "    )\n",
    "\n",
    "print(\"✅ MCP Server started on http://127.0.0.1:8000\")\n",
    "time.sleep(8)  # Give server time to start\n",
    "\n",
    "res = requests.get(\"http://127.0.0.1:8000/\")\n",
    "print(\"✅ Response:\", res.status_code)\n",
    "print(res.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ping MCP server from Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "Two methods are provided to query the associated MCP server.\n",
    "\n",
    "### Option 1 Instructions (query the MCP server from this notebook): \n",
    "1. Run the below cell.\n",
    "\n",
    "### Options 2 Instructions (query the MCP server from CLI, Powershell for Windows):\n",
    "1. Once MCP server is running, open a powershell terminal (or equivalent). \n",
    "2. Navigate to the application root directory, /weatherChatbot.\n",
    "3. In applicable, start your virtual environment: .venv\\Scripts\\Activate.ps1\n",
    "\n",
    "There are two options to run for the CLI Agent: \n",
    "- \"cli_agent/main.py\": this is the same script that runs in the Option 1 below. It is a very simple intake from the CLI, with no error handling, conversation, etc.\n",
    "- \"cli_agent/main-enhanced.py\": as stated in the title, this version of the cli_agent is enhanced with an LLM back-end, to facilitate a conversation and more robust handling of input variation. A few important notes: although the code is written to first check for an OpenAI endpoint and use a local model if not (and download if necessary), the local model interaction is very slow and does not provide robust responses. The code has been written to leverage the OpenAI endpoint (even if the original key is commented out, see comments in .env file) to facilitate UX.\n",
    "\n",
    "Make a selection and run using the below steps.\n",
    "\n",
    "4. Run CLI agent with following command: python cli_agent/main.py OR python cli_agent/main-enhanced.py\n",
    "5. Dialogue asking for city input should appear. Enter desired city (within the US) with the following format: City, State Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'New York, NY', 'forecast': 'City: New York, NY  \\nLatitude: 40.7127281  \\nLongitude: -74.0060152  \\nNOAA Station ID: GHCND:USW00094728  \\n\\n| Date                | 2025-06-02 | 2025-06-03 | 2025-06-04 | 2025-06-05 | 2025-06-06 | 2025-06-07 | 2025-06-08 |\\n|---------------------|------------|------------|------------|------------|------------|------------|------------|\\n| Max Temp (°C)       | 22.0       | 25.5       | 24.5       | 29.5       | 28.7       | 24.0       | 24.6       |\\n| Min Temp (°C)       | 12.2       | 13.4       | 15.7       | 19.5       | 21.5       | 19.6       | 18.1       |\\n| Precipitation (mm)  | 0.0        | 0.0        | 0.0        | 1.0        | 2.8        | 8.5        | 0.55       |\\n| Max Wind Speed (m/s)| 8.8        | 8.2        | 13.3       | 14.8       | 12.4       | 10.7       | 8.9        |\\n\\nCommentary:\\n\\n**Historical comparison:**\\n- The forecasted maximum temperatures for this week are slightly below the historical mean for early June, indicating a cooler trend compared to past data.\\n- The predicted precipitation levels on June 6 and June 7 are significantly higher than historical averages, suggesting potential unusual weather patterns.\\n- Wind speeds are expected to be slightly higher than typical historical values, especially on June 4 and June 5.\\n\\n**Important considerations:**\\n- Residents should be prepared for potentially heavy rainfall on June 7, which could lead to localized flooding in susceptible areas.\\n- With warmer temperatures persisting, ensure proper hydration and avoid prolonged exposure to direct sunlight.\\n- Be cautious of gusty winds, especially on June 4 and June 5, when engaging in outdoor activities.\\n\\n**About the data:**\\n- The forecast combines data from Open Meteo and Weatherbit, averaging values where discrepancies existed.\\n- Significant differences in wind speed predictions were noted between data sources, resolved by selecting a median value.\\n- Precipitation discrepancies on June 7 were averaged due to the large difference between data sources.\\n\\n**Weather-related news:**\\n- Recent news highlights a wet weather pattern developing in the region, with potential for thunderstorms later in the week.\\n- Article 1: \"New York Prepares for Heavy Rainfall Later This Week\" - [link]\\n- Article 2: \"Cooler Temperatures Expected in NYC Amid Summer\" - [link]\\n- Article 3: \"Gusty Winds and Rain Expected in the Northeast\" - [link]\\n\\n**Social media posts:**\\n- Social media reflects mixed sentiments, with some expressing concern over potential flooding, while others welcome cooler temperatures.\\n- Post 1: \"Look out for heavy rain next week in NYC, might want to keep an umbrella handy! #NYCWeather\" - [link]\\n- Post 2: \"Finally, some relief from the heat with cooler temps expected in NY next week!\" - [link]\\n- Post 3: \"Windy days coming up! Secure any outdoor furniture and stay safe #NYWeather\" - [link]\\n\\nThank you for your attention, and please stay informed with local weather updates for any changes.'}\n"
     ]
    }
   ],
   "source": [
    "# Query MCP server from jupyter notebook\n",
    "\n",
    "# Input city name , with corresponding USW station for reference. Actual station used may differ.\n",
    "city = \"New York, NY\" #GHCND:USW00094728\n",
    "\n",
    "# Obtain response\n",
    "response = requests.post(\"http://127.0.0.1:8000/forecast\", json={\"city\": city})\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close MCP server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "When finished, terminate all processes associated with application.\n",
    "\n",
    "### Instructions: \n",
    "1. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 MCP Server stopped.\n"
     ]
    }
   ],
   "source": [
    "# Terminate server when finished\n",
    "server.terminate()\n",
    "server.communicate()\n",
    "time.sleep(8)  # Give server time to stop\n",
    "print(\"🛑 MCP Server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (weatherChatbot)",
   "language": "python",
   "name": "weatherchatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
